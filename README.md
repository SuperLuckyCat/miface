# miface
Facial Expression Mapping

Miface is an ongoing research and software development project meant to expand the repertoire of facial expressions that can be automatically detected by recognition systems, and emulated by 3D virtual humans.

The miface pipeline includes modeling base expressions in faceshift, then exporting them as fbx files to Maya where the resulting blendshapes are tuned to align their surface deformations with descriptions and examples of Action Units (AUs) provided by the Facial Action Coding System (FACS: Ekman, Friesen and Hager, 2002). Additional blendshapes have been added to complete the set of AUs essential for modeling the majority of facial expressions.

Code written in Maya Embedded Language (MEL) generates a range of parameterized facial movement configurations. Several studies, using crowdsourced participants from Amazon's Mechanical Turk, have provided label sets for several hundred potential discrete expressions. At this time, the research focus is on determining whether using cosine similarity for word vector comparison provides a good distance metric for identifying the best single label out of a set for an expression. Previously, the Lesk algorithm in Wordnet Similarity by Ted Pederson was used as a synonymy weighting tool and the sum of weights was used to find the semantic centroid of a label set (if one exists). However, limitations of that method have led to further testing of alternate means.

MEL scripts have also been coded to generate animations based on the AU output of the commercial facial expression recognition software, FaceReader by VicarVision. FaceReader has been trained using traditional machine learning methods such as active appearance models, which require supervised learning based on a large set of images annotated by trained FACS coders. The miface database will provide a novel means of expression recognition that relies on the motion capture and animation technique of retargeting, in which motion capture data obtained from a human actor is applied to an animated characted, which imitates the actor's movements. Because miface can map retargeting data to the weighting parameters of an animated character's blendshapes, or morph targets, a lookup table of known weighting configurations with their associated labels can be used for expression identification, obviating the need for training. The FaceReader test was conducted to see how closely their AU output mimicked the human actor's performance as compared to the retartegting feature built into faceshift. Limitations to the accuracy of current retargeting technology will affect the performance of miface as a recognition tool, but improvements in this field are occuring rapidly.

The code and other files contained in this repository represent the programs, input, and output files used to perform the experiments and tests detailed above, with the exception of the proprietary miface Maya model and the blendshape parameter/label sets that make up the miface "facebase". The experimental method is patent pending.
