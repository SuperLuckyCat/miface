# MiFace
Facial Expression Mapping

MiFace is an ongoing research and software development project designed to expand the repertoire of facial expressions mapped to quantified movements in the human face, along with natural language labels denoting their associated social signal values. Applications of a rich facial expression lexicon include improving the servicibility of automatic recognition systems, supporting the development of affective virtual humans, and diagnostics and treatment in psychology. Using a 3D model of the human face capable of emulating the muscle activations, specifically the Action Units (AUs) provided by the Facial Action Coding System (FACS: Ekman, Friesen and Hager, 2002), a broad range of potential expressions can be rapidly and reliably generated for testing.

Code written in Maya Embedded Language (MEL) generates a range of parameterized facial movement configurations. Several studies, using crowdsourced participants from Amazon's Mechanical Turk, have provided label sets for several hundred potential discrete expressions based on these configurations. Processing of the response sets centers around identifying a semantic centroid, if one exists. This determination is made by calculating all-pairs cosine similarity scores using GloVe, then performing a cluster analysis using Matlab. If a reliable centroid is found, it is designated the primary label for an expression.

The code and other files contained in this repository represent the scripts and programs used to perform the data processing and analysis detailed above, with the exception of the proprietary MiFace Maya model and the blendshape parameter/label sets that make up the MiFace "facebase". Many of the bash scripts perform a single step in data cleaning or preparation.
